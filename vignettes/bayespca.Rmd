---
title: "bayespca Package"
author: "Davide Vidotto <d.vidotto@uvt.nl>"
geometry: margin=1cm
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    toc: true
  pdf_document:
    toc: true
  github_document:
    toc: true
vignette: >
  %\VignetteIndexEntry{bayespca Package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


# bayespca: A package for Variational Bayes PCA

## Theoretical background
Principal Components Analysis (PCA) allows performing dimensionality reduction via matrix factorization.
While there are several ways to express a PCA model, in what follows will we consider the formulation
$$ X = X W P^T + E, $$
where X is a $I \times J$ data matrix ($I$ is the number of units; $J$ the number of
continuous variables); $W$ is a $J \times D$ weight matrix ($D \leq J$ is the rank of the reduced matrix);
$P$ is the orthogonal loading matrix, such that $P^T P = I_{D \times D}$; and $E$ is an $I \times J$ error matrix. The $D$ principal components can be retrieved with $Z = X W$.
In this context, the focus of the inference is typically on $W$. In particular, when $J$ is large and the main inferential goal is components' interpretation, it is important for the analyst to obtain simple and interpretable components.

The `bayespca` package allows performing the following operations:

1. estimation of the PCA model, with a Variational Bayes algorithm;   
1. regularization of the elements of $W$ by means of its prior variances;
1. variable selection, via automatic relevance determination (ARD).


The Variational Bayes algorithm sees the columns of $W$ as latent variables, and $P$ as a fixed parameter. Furthermore, the residuals $E$ are assumed to be distributed according to a Normal distribution with mean 0 and variance $\sigma^2$. The following prior is assumed for the $d$-th column of $W$:

$$ w_d \sim MVN(0, T_d^{-1})  $$

where $MVN()$ denotes the density of the Multivariate Normal Matrix, and $T_d$ denotes the prior (diagonal) precision matrix of the $d$-th component. The $j$-th element of the diagonal of $T_d$ will be denoted $\tau_{dj}$.  


## The `bayespca` package
Variational Bayes PCA is implemented through the ```vbpca``` function, which takes the following arguments as inputs:

* ```X``` the input matrix;
* ```D``` the number of components to be estimated;
* ```nstart``` number of times a different run of the algorithm is performed (with varying starting values);
* ```maxIter``` the maximum number of iterations for the Variational Bayes algorithm;
* ```tolerance``` convergence criterion of the algorithm (relative difference between ELBO values);
* ```verbose``` logical parameter which prints estimation information on screen when ```TRUE```;
* ```center``` boolean indicating whether to center the variables in X;
* ```scalecorrection```, a float which is >= than 0 if the variables are scaled (by a factor ```scalecorrection```), and <0 otherwise;
* ```svdStart```, a boolean denoting whether to use the values of SVD decomposition for the starting values (opposed to random starts);
* ```tau``` value of the prior precisions; starting value when ```updatetau=TRUE``` or ```alphatau > 0``` (for Gamma priors)
* ```updatetau``` logical parameter denoting whether the prior precisions should be updated when ```priorvar='fixed'```;
* ```alphatau``` values of the shape parameter of the Gamma priors for the precisions; prior precisions are fixed when set to 0
* ```betatau``` values of the scale parameter of the Gamma priors for the precisions;
* ```plot.lowerbound``` boolean indicating whether to plot the history of the ELBO values calculated during the variational iterations;    
* ```hpdi``` logical indicating whether to calcualte the HPD intervals of the weights;
* ```probHPDI``` the probability density covered by the HPDI's;   
* ```global.var``` logical parameter which activates component-specific prior variances when set to ```TRUE```;

```vbpca``` returns a vbpca object, which is a list containing various aspect of the model results. See ```?vbpca``` for further information. Internally, ```vbpca``` calls a C++ function (written with Rcpp) to estimate the model. When ```nstart```>1, the algorithm will autmatically pick (and output) the best
run in terms of final ELBO value.

In what follows, the various estimation modalities allowed by ```vbpca``` will be introduced. For presentation purposes, a synthetic data matrix with $I = 100$ rows and $J = 20$ columns genereted from three components will be used:

```{r}
set.seed(141)
I <- 100
J <- 20
V1 <- rnorm(I, 0, 50)
V2 <- rnorm(I, 0, 30)
V3 <- rnorm(I, 0, 10)
X <- matrix(c(rep(V1, 7), rep(V2, 7), rep(V3, 6)), I, J)
X <- X + matrix(rnorm(I * J, 0, 1), I, J)
```


I will now proceed with the estimation of the PCA model.

## Levels of regularization on the W matrix
### Fixed ```tau```
With fixed tau, it is possible to specify the model as follows:

```{r}
# Install and load package
# devtools::install_github("davidevdt/bayespca")
library(bayespca)

# Estimate vbpca with fixed prior precisions (equal to 1)
# for the elements of W
mod1 <- vbpca(X, D = 3, maxIter = 1e+03, alphatau=0,
              center = FALSE, scalecorrection = -1,
              plot.lowerbound = FALSE,
			  verbose = FALSE )

# Test the class of mod1:
is.vbpca(mod1)


```

The estimate posterior means of the  $W$ matrix can be viewed with:

```{r}
mod1$muW
```

and the $P$ matrix:

```{r}
mod1$P
```

Among other things, the function returns the model evidence lower bound (ELBO) and the estimation time:

```{r}
mod1$elbo

mod1$time
```


### Fixed, updatable ```tau```
The prior precisions $\tau_{dj}$ can also be updated via Type-II Maximum Likelihood (empirical Bayes updates):

```{r}
mod2 <- vbpca(X, D = 3, maxIter = 1e+03, alphatau=0,
             updatetau = TRUE, center = FALSE,
             scalecorrection = -1,
             plot.lowerbound = FALSE,
             verbose = FALSE )

mod2$muW
```

The matrix of the prior precisions can be called with

```{r}
mod2$Tau
```

### Random ```tau```: Gamma prior
It is possible to specify a gamma prior on $\tau_{d,j}$:

$$ \tau_{d,j} \sim G(\alpha, \beta) $$

with $\alpha$ shape parameter and $\beta$ scale parameter. The following code implements an IG(2, .5) prior on the precisions:


```{r}
# Estimate the model
mod3 <- vbpca(X, D = 3, maxIter = 1e+03,
              alphatau = 2, betatau = .5,
              center = FALSE, scalecorrection = -1,
              plot.lowerbound = FALSE,
              verbose = FALSE )
mod3$muW


mod3$Tau
```


```alphatau``` and ```betatau``` can also be specified as $D$-dimensional array, in which case the Gamma will have
component-specific hyperparameters:
$$ \tau_{d,j} \sim G(\alpha_d, \beta_d) $$.


```{r}
# Estimate the model
mod4 <- vbpca(X, D = 3, maxIter = 1e+03,
              center = FALSE, scalecorrection = -1,
              alphatau = c(.5, 50, 3), betatau = c(.5, .01, 10),
              plot.lowerbound = FALSE,
              verbose = FALSE )

mod4$muW

mod4$Tau
```

### Global prior variances
So far, the parameter ```global.var``` has always ben set to ```FALSE```, implying
$$ w_{j,d} \sim N(0, \tau_{j,d}^{-1}). $$
Setting ```global.var = TRUE``` will modify this formulation, which will switch to
$$ w_{j,d} \sim N(0, \tau_{d}{-1}) $$
that is, component-specific variances (called 'global variances' in ```vbpca```) will be estimated instead:

```{r, fig.cap = "Prior precisions for the first 3 components."}
# Fixed prior global variances, updated via Type-II maximum likelihood:
mod5 <- vbpca(X, D = 3, maxIter = 1e+03, alphatau=0,
              updatetau = TRUE,  
              center = FALSE, scalecorrection = -1,
              plot.lowerbound = FALSE,
              verbose = FALSE, global.var = TRUE)

mod5$muW


mod5$Tau
```

Notice the plot of the precisions that appears in this case. This is useful when
the number of components supported by the data is uncertain (scree-plot - see Figure 2):


```{r, fig.cap = "Scree-plot for 10 components. "}
mod6 <- vbpca(X, D = 10, maxIter = 1e+03, alphatau=0,
              updatetau = TRUE,  
              center = FALSE, scalecorrection = -1,
              plot.lowerbound = FALSE,
              verbose = FALSE, global.var = TRUE)

mod6$Tau
```

## Automatic Relevance Determination
When the prior precisions are updated, they can help to perform component-specific
variable selection through Automatic Relevance Determination (ARD). In particular,
values in the Tau matrix that are extremely large determine the values of the
weights that can be set to 0. This is because their inverse (prior variances) are very close to 0,
and thus making the elements of $W$ also close to 0 with high probability. We're going to show an example here,
with fixed (updated through Type-II maximum likelihood) precisions (in case of Gamma prior, it is recommended to use
hyperparamter values close to 0).


```{r}
mod7 <- vbpca(X, D = 3, maxIter = 1e+03, alphatau=0,
              updatetau = TRUE, center = FALSE,
              scalecorrection = -1,
              plot.lowerbound = FALSE,
              verbose = FALSE)

mod7$muW


mod7$Tau
```

We can also plot an heatmap of the resulting precision matrix; we set a threshold parameter (to establish when the
precision elements are too large) equal to 50:

```{r, fig.cap = "Heatmap of Tau. ", fig.width=5, fig.height=5.5}
mat_mod_7 <- plotheatmap(mod7, matrix_type="Tau", bound_tau=50)
mat_mod_7$W
mat_mod_7$Tau 
```


## High posterior density intervals
It is also possible to require the computation of high probability density intervals for the elements of $W$, which can then be plotted with the ```plothpdi``` function, which internally calls ```ggplot2``` functionalities. *Note*: when the weights are required in normalised form, the posterior density interval will still be returned in the original weights scale (thus, no normalisation is performed on the HPDIs).

```{r, fig.show='hold', fig.cap = "High posterior density intervals. ", fig.width=4, fig.height=6 }
# Set hyperparameter values and require 90% probability density intervals
# Estimate the model
mod8 <- vbpca(X, D = 3, maxIter = 1e+03,
                alphatau = .001, betatau = .001,
                center = FALSE, scalecorrection = -1,
                hpdi = TRUE, probHPDI = 0.9,
                plot.lowerbound = FALSE,
                verbose = TRUE )

# Plot HPD intervals for variables 1:10, component 1
plothpdi(mod8, d = 1, vars = 1:20)
```


## Retrieve Principal Components
To compute the estimated components, simpy call:


```{r}
PCs <- X %*% mod1$muW 
head(PCs, 15)
```


### References
 1. C. M. Bishop. 'Variational PCA'. In Proc. Ninth Int. Conf. on Artificial Neural Networks. ICANN, 1999.
